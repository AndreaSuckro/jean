\graphicspath{ {imgs/} }
\documentclass[main.tex]{subfiles}
\begin{document}
\chapter{Bandit AB-Testing}
\section{Underlying Idea}
The previous chapter dealt with optimization concerning classical AB-Testing. The following will describe a fundamentally different approach in which there is no separation between the exploration and the exploitation phase. This is motivated by the idea that valuable costumers could be lost during exploration. Basically this means that we want to already maximize our profit while gaining information about the other buckets. When combining exploration and exploitation one has to choose on each step if a known good bucket should be used again or if one invests in other unknown buckets to gain more information about their reward function. This balancing is at the core of the now described algorithms.

\subsection{Bandit Algorithms}
Bandit algorithms originate from Machine Learning where they serve learning agents to show sensible behavior while exploring the environment. In each time step an agent is forced to make a choice among several actions. An action will lead to a reward. The reward function for any action is not known from the beginning and the agent can only estimate the true reward function of any action by trying this action. The objective is of course to maximize the received reward over the whole experiment. The name of this class of algorithms stems from their relation to the famous casino slot machines. Each action can be imagined as one lever in a row of many one-armed bandits. Each machine has a hidden reward function and a player wants to maximize their earned money at the end of the evening. The update rule for each step can be roughly formalized as:
\begin{align*}
NewEstimate \leftarrow OldEstimate + StepSize[Target - OldEstimate]
\end{align*}
A K-armed bandit problem is defined by random variables $X_{i,n}$ for $1 \leq i \leq K$ and $n\geq1$ where each $i$ is the index of a slot machine. Successive plays of machine i yield reward $X_{i,1},X_{i,2},...$ which are independent and identically distributed according to an unknown law with unknown expectation $\mu_i$. The machines are also independent.

\subsection{Reformulating AB-Testing to Bandits}
Each new user is our chance to either explore or exploit what we know about the buckets of the test. Each assignment becomes the pull on a lever. Since we only measure each user once, the assignments are independent and identically distributed. We deal with a stationary environment which means our $StepSize$ is $\frac{1}{k}$ where $k$ denotes the number of assignments. Since we use no prior knowledge there is also \emph{no side information}. 
\subsubsection{Terms}
We want to find the real distributions underlying the variations we made in the buckets. Assume this distributions to be of the form $P_B(y|w)$ where $w$ are the unknown parameters that describe the distribution. By assigning users we generate random observations $D_B=(y_1,y_2..y_t)$.

\section{Bandit algorithms}
\subsection{epsilon-greedy}
One of the simplest algorithms exploits always the best performing bucket (with the highest conversion rate) except for $\epsilon $'s fraction of cases where the next bucket is chosen uniformly. For example: if $\epsilon = 0.1$ every tenth assignment would not be to the best performing bucket. This also means that even after convergence for the bucket probabilities $10\%$ of the assignments would not always hit the optimal bucket. For $n$ buckets this would be:
\begin{align*}
P(exploration) \cdot P(\neg best machine | exploration) = \epsilon \cdot \frac{n-1}{n}
\end{align*}
times a not optimal result. A simulation with 1000 times 1000 assignments and varying $\epsilon$ gives the following plot for this algorithm:
\begin{figure}[ht]
\includegraphics[scale=0.5]{epsGreedy.png}
\centering
\title{Epsilon Greedy}
\end{figure}
An epsilon-greedy setting would still make sense, if the environment changes and users behavior change later on.

\subsection{epsilon-first}
Another algorithm that is closely related is called the $\epsilon-first$ algorithm. This is close to A/B-Testing since the exploration phase proceeds the exploitation phase for a finite number of steps and afterwards just exploitation happens. This would mimic the case where the tested change with the highest payoff is implemented and from then on permanently presented to all customers.
\begin{figure}[ht]
\includegraphics[scale=0.5]{epsFirst.png}
\centering
\title{Epsilon First}
\end{figure}

\subsection{UCB}
This algorithm does not explore for a fixed number of assignments, but dynamically changes the chances for not so good performing buckets to be chosen. The Hoeffding's inequality gives a confidence bound that models how sure we are about the estimated payoff across multiple plays. This way - the longer we do not play a machine the more likely it will be played in the future (but only once). The probability for each machine is defined as:
\begin{align*}
mean(b_n) + \sqrt{\frac{2ln(N)}{n_p(b_n)}}
\end{align*}
Here $b_n$ is one of $n$ buckets and $n_p(b_n)$ is the number of times this bucket has been played. $N$ corresponds to the number of assignments across all buckets. This algorithm requires $mean(b_n)$ to be bounded by 1 so that the confidence bound overpowers the first term. The fraction of time that is spend on exploring decreases exponentially.

\subsection{Thompson sampling}
This fairly simple algorithm was originally developed by Wiliam R. Thompson in his paper "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples"
To determine the next bucket to be played, the buckets probability distribution itself is used.

They generate numbers that naturally balance between exploitation and exploration, because less performing buckets are demoted and winners promoted by the information already collected.

\subsection{Discussion}
bandit algorithms are ment do be long running.

\end{document}