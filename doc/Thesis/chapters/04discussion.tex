\documentclass[main.tex]{subfiles}
\begin{document}
\chapter{Discussion}
Although Bandit and regular A/B Testing algorithms have a lot in common they do not really solve the same problems which makes a clear decision upon which is better hard. The Bandit algorithms make more sense in an ongoing test setting, where the surrounding conditions can change through the trial and a test is not finished after a fixed short time period. This also entails that users have to cope with possibly more changes from time to time. In fact when coming back to the limitations that were set at the beginning of the thesis more points come up that may cause troubles.

\section{The Limitations of the Used Restrictions}
In the beginning of the thesis we applied some simplifications to the problem of A/B Testing. The number of buckets was restricted to two which is not harmful since no algorithm takes this as a hard precondition, although it can affect optimization \ref{ssec:analytic}. This means that the described methods work for more buckets as well. Specifically for the Bandit algorithms it makes sense to use UCB or Thompson sampling with more buckets, since they should outperform epsilon-first and epsilon-greedy in this setting.

Another factor is that for a realistic A/B Test it is not clear when the user is producing an event. She may come to the tested page but not click the specified link or button. This means that until the user really performs an action we can only speak of a non-click event with a certain probability. For modeling this probability data from previous tests could be utilized, making it more and more likely that the user will not click as more time passes by. Users can also provide more than one event, making the samples not independent anymore. It is not really clear how the Bandit algorithms have to be adapted in a way that they support this setting.

The described methods make not use of a prior so far. Using previous test runs could provide a distribution over the differences between buckets. This distribution is most likely shaped similar to an exponential decay function, making small differences between buckets more likely than huge differences. This makes sense, because no introduced change to a certain product will profoundly change the users behavior. This prior can be easily integrated in the Analytic and Bandit's approaches, since the used beta distributions would not start with a flat prior, but the values estimated from previous similar test runs.

\section{Duration of the Test}
Just comparing the duration of an A/B Test, one could argue that Bandit algorithms are useful if they keep on running in the background of a product/service and not turned off at a certain time, hence rendering the question for statistical significance useless and since they are adapting dynamically one could react to changes occurring later in time. This is a theoretical solid idea, but it leads also to several implications that are possibly not easy to be resolved: 

Imagine a reasonable sized application or service. If testing proofs valuable this application will not just have one test running at a time. The different buckets have to be maintained in the code and dragged along through several releases as some customers might still get that experience. Which brings up another point: if the Bandit A/B Test finds that one bucket is clearly favorable over the other -- why keep it from a small percentage of the over all user base?

This affects not only users but customer service in a running business. How to handle requests that are made upon different buckets by the user? The tests themselves may not drastically change the application, but many little tests can blur the whole picture.

No statistical model can compensate for a test that is not run long enough to cover different behavioral patterns of the targeted users during time. Meaning that for any useful test the cycles that are important have to be identified first (work-weekend, day-night, differences in timezone) and the duration adapted to account for those.

\section{Final thoughts}
Given the discussed restrictions of the evaluated Bandit algorithms I think it is to early to adopt them to A/B Testing. In real world applications it makes sense to have the tests bound to a certain time interval and later on deciding for one feature to be rolled out to all the customers. Having said that, I also think, that using the normal approximation as it is done so far in many A/B Testing systems is not good either. The proposed model with Beta distributions could be easily used to incorporate data from previous A/B Tests, which are unaccounted for in the regular setting. The interpretation of the test results would also become more easy since the concrete probabilities can be reported to the experimenter and she can then decide (given some reasonable extra assumptions, that can not be accounted for by the theory - like differences between day and night, workday or weekend) when the test is over.

% the end :)
\end{document}